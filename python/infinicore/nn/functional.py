import infinicore
from infinicore.lib import _infinicore
from infinicore.tensor import Tensor
import math

__all__ = ["causal_softmax", "rms_norm", "silu", "swiglu"]


def causal_softmax(input: Tensor, out=None) -> Tensor:
    r"""Apply a causal softmax function."""

    if out is None:
        return Tensor(_infinicore.causal_softmax(input._underlying))

    _infinicore.causal_softmax_(out._underlying, input._underlying)

    return out


def rms_norm(
        input: Tensor,
        normalized_shape: list[int],
        weight: Tensor,
        eps: float = 1e-5,
        *,
        out=None,
) -> Tensor:
    r"""Apply Root Mean Square Layer Normalization."""

    assert normalized_shape == weight.shape, (
        "normalized_shape does not match weight.shape."
    )

    if out is None:
        return Tensor(_infinicore.rms_norm(input._underlying, weight._underlying, eps))

    _infinicore.rms_norm_(out._underlying, input._underlying, weight._underlying, eps)

    return out


def silu(input: Tensor, inplace: bool = False, *, out=None) -> Tensor:
    r"""Apply the Sigmoid Linear Unit (SiLU) function, element-wise."""

    if infinicore.use_ntops and input.device.type in ("cuda", "musa") and out is None:
        return infinicore.ntops.torch.silu(input, inplace=inplace)

    if inplace:
        _infinicore.silu_(input._underlying, input._underlying)
        return input

    if out is None:
        return Tensor(_infinicore.silu(input._underlying))

    _infinicore.silu_(out._underlying, input._underlying)

    return out


def swiglu(input: Tensor, other: Tensor, *, out=None):
    r"""Apply the Swish-Gated Linear Unit (SwiGLU) function, element-wise."""

    if out is None:
        return Tensor(_infinicore.swiglu(input._underlying, other._underlying))

    _infinicore.swiglu_(out._underlying, input._underlying, other._underlying)

    return out


def linear(input: Tensor, weight: Tensor, bias=None, *, out=None) -> Tensor:
    r"""Applies a linear transformation to the incoming data: y=xA^T+b."""

    if out is None:
        return Tensor(_infinicore.linear(input._underlying,
                                         weight._underlying,
                                         None if bias is None else bias._underlying
                                         )
                      )

    _infinicore.linear_(out._underlying,
                        input._underlying,
                        weight._underlying,
                        None if bias is None else bias._underlying
                        )
    return out


def embedding_bk(
        input: Tensor,
        weight: Tensor,
        padding_idx: int = None,
        max_norm: float = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
        *,
        out=None
) -> Tensor:
    r"""Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.
    """
    assert (padding_idx == None) and (max_norm == None) and (scale_grad_by_freq == False) and (sparse == False), "Unsupported parameters."

    if out is None:
        return Tensor(_infinicore.embedding(input._underlying, weight._underlying))

    _infinicore.embedding(out._underlying, input._underlying, weight._underlying)

    return out


def embedding(input: Tensor, weight: Tensor, *, out=None) -> Tensor:
    r"""Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.
    """

    if out is None:
        return Tensor(_infinicore.embedding(input._underlying, weight._underlying))

    _infinicore.embedding_(out._underlying, input._underlying, weight._underlying)
    return out


def swiglu(input: Tensor, other: Tensor, *, out=None):
    r"""Apply the Swish-Gated Linear Unit (SwiGLU) function, element-wise."""

    if out is None:
        return Tensor(_infinicore.swiglu(input._underlying, other._underlying))

    _infinicore.swiglu_(out._underlying, input._underlying, other._underlying)

    return out


def rope(x: Tensor,
         pos_ids: Tensor,
         sin_table: Tensor,
         cos_table: Tensor,
         algo: _infinicore.Algo,
         *,
         out=None) -> Tensor:
    r"""
    Rotary Position Embedding(RoPE).
    """
    if out is None:
        return infinicore.Tensor(
            _infinicore.rope(x._underlying, pos_ids._underlying, sin_table._underlying, cos_table._underlying, algo)
        )

    _infinicore.rope_(
        out._underlying, x._underlying, pos_ids._underlying, sin_table._underlying, cos_table._underlying, algo
    )
    return out
