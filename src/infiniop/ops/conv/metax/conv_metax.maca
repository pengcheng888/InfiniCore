#include "../../../devices/metax/metax_common.h"
#include "../../../devices/metax/metax_handle.h"
#include "conv_metax.h"

#define DESTROY_MCDNN_DESCRIPTOR(desc_ptr, destroy_func) \
    do {                                                 \
        if (desc_ptr) {                                  \
            destroy_func(desc_ptr);                      \
            desc_ptr = nullptr;                          \
        }                                                \
    } while (0)

#define CLEANUP_MCDNN_DESCRIPTORS()                                             \
    do {                                                                        \
        DESTROY_MCDNN_DESCRIPTOR(x_desc, mcdnnDestroyTensorDescriptor);         \
        DESTROY_MCDNN_DESCRIPTOR(y_desc, mcdnnDestroyTensorDescriptor);         \
        DESTROY_MCDNN_DESCRIPTOR(w_desc, mcdnnDestroyFilterDescriptor);         \
        DESTROY_MCDNN_DESCRIPTOR(b_desc, mcdnnDestroyTensorDescriptor);         \
        DESTROY_MCDNN_DESCRIPTOR(act_desc, mcdnnDestroyActivationDescriptor);   \
        DESTROY_MCDNN_DESCRIPTOR(conv_desc, mcdnnDestroyConvolutionDescriptor); \
    } while (0)

namespace op::conv::metax {

struct Descriptor::Opaque {
    std::shared_ptr<device::metax::Handle::Internal> internal;
    size_t workspace_size = 0;

#ifdef ENABLE_CUDNN_API
    mcdnnTensorDescriptor_t x_desc = nullptr;
    mcdnnTensorDescriptor_t y_desc = nullptr;
    mcdnnFilterDescriptor_t w_desc = nullptr;
    mcdnnTensorDescriptor_t b_desc = nullptr;
    mcdnnActivationDescriptor_t act_desc = nullptr;
    mcdnnConvolutionDescriptor_t conv_desc = nullptr;
    mcdnnConvolutionFwdAlgo_t algo = MCDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;
#endif

private:
    Opaque(std::shared_ptr<device::metax::Handle::Internal> internal_ptr)
        : internal(internal_ptr) {}

#ifdef ENABLE_CUDNN_API
    void initializeDimensionArrays(const ConvInfo &info,
                                   std::vector<int> &input_dims,
                                   std::vector<int> &output_dims,
                                   std::vector<int> &filter_dims,
                                   std::vector<int> &input_strides,
                                   std::vector<int> &output_strides) const {
        bool is_1d_conv = (info.ndim() == 1);
        int actual_tensor_ndim = is_1d_conv ? 4 : static_cast<int>(info.ndim() + 2);

        input_dims[0] = static_cast<int>(info.batch());
        input_dims[1] = static_cast<int>(info.in_channels());
        output_dims[0] = static_cast<int>(info.batch());
        output_dims[1] = static_cast<int>(info.out_channels());
        filter_dims[0] = static_cast<int>(info.out_channels());
        filter_dims[1] = static_cast<int>(info.in_channels());

        if (is_1d_conv) {
            input_dims[2] = 1;
            input_dims[3] = static_cast<int>(info.input_dim(0));
            output_dims[2] = 1;
            output_dims[3] = static_cast<int>(info.output_dim(0));
            filter_dims[2] = 1;
            filter_dims[3] = static_cast<int>(info.kernel_dim(0));
        } else {
            for (size_t i = 0; i < info.ndim(); ++i) {
                input_dims[i + 2] = static_cast<int>(info.input_dim(i));
                output_dims[i + 2] = static_cast<int>(info.output_dim(i));
                filter_dims[i + 2] = static_cast<int>(info.kernel_dim(i));
            }
        }
        calculateStrides(input_dims, input_strides, actual_tensor_ndim);
        calculateStrides(output_dims, output_strides, actual_tensor_ndim);
    }

    void initializeConvolutionParams(const ConvInfo &info,
                                     std::vector<int> &pads,
                                     std::vector<int> &strides,
                                     std::vector<int> &dilations) const {
        bool is_1d_conv = (info.ndim() == 1);

        if (is_1d_conv) {
            pads[0] = 0;
            pads[1] = static_cast<int>(info.pad_info(0));
            strides[0] = 1;
            strides[1] = static_cast<int>(info.stride_info(0));
            dilations[0] = 1;
            dilations[1] = static_cast<int>(info.dilation_info(0));
        } else {
            for (size_t i = 0; i < info.ndim(); ++i) {
                pads[i] = static_cast<int>(info.pad_info(i));
                strides[i] = static_cast<int>(info.stride_info(i));
                dilations[i] = static_cast<int>(info.dilation_info(i));
            }
        }
    }

    void calculateStrides(const std::vector<int> &dims,
                          std::vector<int> &strides,
                          int ndim) const {
        strides[ndim - 1] = 1;
        for (int d = ndim - 2; d >= 0; --d) {
            strides[d] = strides[d + 1] * dims[d + 1];
        }
    }

    infiniStatus_t getMcdnnDataType(infiniDtype_t data_type,
                                    mcdnnDataType_t &mcdnn_data_type) const {
        if (data_type == INFINI_DTYPE_F16) {
            mcdnn_data_type = device::metax::getHcdnnDtype(data_type);
        } else if (data_type == INFINI_DTYPE_F32) {
            mcdnn_data_type = device::metax::getHcdnnDtype(data_type);
        } else if (data_type == INFINI_DTYPE_BF16) {
            mcdnn_data_type = device::metax::getHcdnnDtype(data_type);
        } else {
            return INFINI_STATUS_BAD_TENSOR_DTYPE;
        }
        return INFINI_STATUS_SUCCESS;
    }

    infiniStatus_t createBasicDescriptors(const std::vector<int> &input_dims,
                                          const std::vector<int> &output_dims,
                                          const std::vector<int> &filter_dims,
                                          mcdnnDataType_t mcdnn_data_type,
                                          int actual_tensor_ndim) {
        CHECK_MCDNN(mcdnnCreateTensorDescriptor(&x_desc));
        CHECK_MCDNN(mcdnnCreateTensorDescriptor(&y_desc));
        CHECK_MCDNN(mcdnnCreateFilterDescriptor(&w_desc));
        CHECK_MCDNN(mcdnnCreateConvolutionDescriptor(&conv_desc));

        CHECK_MCDNN(mcdnnSetTensorNdDescriptorEx(
            x_desc, MCDNN_TENSOR_NCHW, mcdnn_data_type,
            actual_tensor_ndim, input_dims.data()));
        CHECK_MCDNN(mcdnnSetTensorNdDescriptorEx(
            y_desc, MCDNN_TENSOR_NCHW, mcdnn_data_type,
            actual_tensor_ndim, output_dims.data()));
        CHECK_MCDNN(mcdnnSetFilterNdDescriptor(
            w_desc, mcdnn_data_type, MCDNN_TENSOR_NCHW,
            actual_tensor_ndim, filter_dims.data()));

        return INFINI_STATUS_SUCCESS;
    }

    infiniStatus_t createBiasDescriptors(const ConvInfo &info,
                                         mcdnnDataType_t mcdnn_data_type,
                                         int actual_tensor_ndim) {
        if (info.bias_dims_size() == 0) {
            b_desc = nullptr;
            act_desc = nullptr;
            return INFINI_STATUS_SUCCESS;
        }

        std::vector<int> bias_dims_arr(actual_tensor_ndim);
        std::vector<int> bias_strides_arr(actual_tensor_ndim);

        bias_dims_arr[0] = 1;
        bias_dims_arr[1] = static_cast<int>(info.out_channels());
        for (int i = 2; i < actual_tensor_ndim; ++i) {
            bias_dims_arr[i] = 1;
        }

        if (actual_tensor_ndim == 4) {
            bias_strides_arr[0] = static_cast<int>(info.out_channels());
            bias_strides_arr[1] = 1;
            bias_strides_arr[2] = 1;
            bias_strides_arr[3] = 1;
        } else {
            calculateStrides(bias_dims_arr, bias_strides_arr, actual_tensor_ndim);
        }

        CHECK_MCDNN(mcdnnCreateTensorDescriptor(&b_desc));
        CHECK_MCDNN(mcdnnSetTensorNdDescriptor(
            b_desc, mcdnn_data_type, static_cast<int>(bias_dims_arr.size()),
            bias_dims_arr.data(), bias_strides_arr.data()));
        CHECK_MCDNN(mcdnnCreateActivationDescriptor(&act_desc));
        CHECK_MCDNN(mcdnnSetActivationDescriptor(
            act_desc, MCDNN_ACTIVATION_IDENTITY, MCDNN_NOT_PROPAGATE_NAN, 0.0));

        return INFINI_STATUS_SUCCESS;
    }

    infiniStatus_t setupConvolutionDescriptor(const std::vector<int> &pads,
                                              const std::vector<int> &strides,
                                              const std::vector<int> &dilations,
                                              int spatial_ndim,
                                              mcdnnDataType_t compute_type) {
        CHECK_MCDNN(mcdnnSetConvolutionNdDescriptor(
            conv_desc,
            spatial_ndim,
            pads.data(),
            strides.data(),
            dilations.data(),
            MCDNN_CROSS_CORRELATION,
            compute_type));

        return INFINI_STATUS_SUCCESS;
    }

    infiniStatus_t setupAlgorithmWithoutBias() {
        // 尝试使用算法查找API来获取支持的算法
        int maxAlgoCount = 0;
        
        // 为海光DCU提供特殊处理 - 避免使用不支持的API
        CHECK_STATUS(internal->useMcdnn(
            nullptr,
            [&](hcdnnHandle_t handle) {
                auto result = mcdnnGetConvolutionForwardAlgorithmMaxCount(handle, &maxAlgoCount);
                if (result != MCDNN_STATUS_SUCCESS) {
                    // 如果海光DCU不支持此API，使用默认值
                    maxAlgoCount = 8;
                }
                return INFINI_STATUS_SUCCESS;
            }));

        if (maxAlgoCount <= 0) {
            maxAlgoCount = 8;
        }

        std::vector<mcdnnConvolutionFwdAlgoPerf_t> perf_results(maxAlgoCount);
        int algoCounts = 0;

        // 为海光DCU提供特殊处理 - 避免使用可能不支持的API
        CHECK_STATUS(internal->useMcdnn(
            nullptr, [&](hcdnnHandle_t handle) {
                auto result = mcdnnFindConvolutionForwardAlgorithm(
                    handle, x_desc, w_desc, conv_desc, y_desc,
                    maxAlgoCount, &algoCounts, perf_results.data());
                if (result != MCDNN_STATUS_SUCCESS) {
                    // 如果海光DCU不支持此API，使用默认算法
                    algoCounts = 1;
                    perf_results[0].algo = MCDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;
                    perf_results[0].status = MCDNN_STATUS_SUCCESS;
                    perf_results[0].time = 0.0f;
                }
                return INFINI_STATUS_SUCCESS;
            }));

        if (algoCounts < 1) {
            // 如果找不到算法，尝试直接使用默认算法
            algo = MCDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;
            CHECK_STATUS(internal->useMcdnn(
                nullptr,
                [&](hcdnnHandle_t handle) {
                    auto result = mcdnnGetConvolutionForwardWorkspaceSize(
                        handle, x_desc, w_desc, conv_desc, y_desc,
                        algo, &workspace_size);
                    if (result != MCDNN_STATUS_SUCCESS) {
                        // 如果默认算法也不支持，尝试其他算法
                        algo = MCDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM;
                        result = mcdnnGetConvolutionForwardWorkspaceSize(
                            handle, x_desc, w_desc, conv_desc, y_desc,
                            algo, &workspace_size);
                    }
                    if (result != MCDNN_STATUS_SUCCESS) {
                        return INFINI_STATUS_BAD_PARAM;
                    }
                    return INFINI_STATUS_SUCCESS;
                }));
            return INFINI_STATUS_SUCCESS;
        }

        // 使用找到的第一个可用算法
        bool algo_found = false;
        for (int i = 0; i < algoCounts; ++i) {
            infiniStatus_t status = internal->useMcdnn(
                nullptr,
                [&](hcdnnHandle_t handle) {
                    auto result = mcdnnGetConvolutionForwardWorkspaceSize(
                        handle, x_desc, w_desc, conv_desc, y_desc,
                        perf_results[i].algo, &workspace_size);
                    if (result == MCDNN_STATUS_SUCCESS) {
                        algo = perf_results[i].algo;
                        return INFINI_STATUS_SUCCESS;
                    }
                    return INFINI_STATUS_BAD_PARAM;
                });
            if (status == INFINI_STATUS_SUCCESS) {
                algo_found = true;
                break;
            }
            // 如果当前算法失败，继续尝试下一个
        }

        if (!algo_found) {
            // 如果所有算法都失败，尝试直接使用默认算法
            algo = MCDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;
            infiniStatus_t status = internal->useMcdnn(
                nullptr,
                [&](hcdnnHandle_t handle) {
                    auto result = mcdnnGetConvolutionForwardWorkspaceSize(
                        handle, x_desc, w_desc, conv_desc, y_desc,
                        algo, &workspace_size);
                    if (result != MCDNN_STATUS_SUCCESS) {
                        // 如果默认算法也不支持，尝试其他算法
                        algo = MCDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM;
                        result = mcdnnGetConvolutionForwardWorkspaceSize(
                            handle, x_desc, w_desc, conv_desc, y_desc,
                            algo, &workspace_size);
                    }
                    if (result != MCDNN_STATUS_SUCCESS) {
                        return INFINI_STATUS_BAD_PARAM;
                    }
                    return INFINI_STATUS_SUCCESS;
                });
            if (status != INFINI_STATUS_SUCCESS) {
                return INFINI_STATUS_BAD_PARAM;
            }
        }

        return INFINI_STATUS_SUCCESS;
    }

    infiniStatus_t setupAlgorithmWithBias() {
        int maxAlgoCount = 0;

        // 为海光DCU提供特殊处理 - 避免使用不支持的API
        CHECK_STATUS(internal->useMcdnn(
            nullptr,
            [&](hcdnnHandle_t handle) {
                auto result = mcdnnGetConvolutionForwardAlgorithmMaxCount(handle, &maxAlgoCount);
                if (result != MCDNN_STATUS_SUCCESS) {
                    // 如果海光DCU不支持此API，使用默认值
                    maxAlgoCount = 8;
                }
                return INFINI_STATUS_SUCCESS;
            }));

        if (maxAlgoCount <= 0) {
            maxAlgoCount = 8;
        }

        std::vector<mcdnnConvolutionFwdAlgoPerf_t> perf_results(maxAlgoCount);
        int algoCounts = 0;

        // 为海光DCU提供特殊处理 - 避免使用可能不支持的API
        CHECK_STATUS(internal->useMcdnn(
            nullptr, [&](hcdnnHandle_t handle) {
                auto result = mcdnnFindConvolutionForwardAlgorithm(
                    handle, x_desc, w_desc, conv_desc, y_desc,
                    maxAlgoCount, &algoCounts, perf_results.data());
                if (result != MCDNN_STATUS_SUCCESS) {
                    // 如果海光DCU不支持此API，使用默认算法
                    algoCounts = 1;
                    perf_results[0].algo = MCDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM;
                    perf_results[0].status = MCDNN_STATUS_SUCCESS;
                    perf_results[0].time = 0.0f;
                }
                return INFINI_STATUS_SUCCESS;
            }));

        if (algoCounts < 1) {
            return INFINI_STATUS_BAD_PARAM;
        }

        for (int i = 0; i < algoCounts; ++i) {
            CHECK_STATUS(internal->useMcdnn(
                nullptr,
                [&](hcdnnHandle_t handle) {
                    CHECK_MCDNN(mcdnnGetConvolutionForwardWorkspaceSize(
                        handle, x_desc, w_desc, conv_desc, y_desc,
                        perf_results[i].algo, &workspace_size));
                    return INFINI_STATUS_SUCCESS;
                }));
            algo = perf_results[i].algo;
            break;
        }

        return INFINI_STATUS_SUCCESS;
    }
#endif

public:
    Opaque(Opaque &&other) noexcept
        : internal(std::move(other.internal)),
          workspace_size(other.workspace_size)
    // clang-format off
#ifdef ENABLE_CUDNN_API
          , x_desc(other.x_desc)
          , y_desc(other.y_desc)
          , w_desc(other.w_desc)
          , b_desc(other.b_desc)
          , act_desc(other.act_desc)
          , conv_desc(other.conv_desc)
          , algo(other.algo)
#endif
    // clang-format on
    {
#ifdef ENABLE_CUDNN_API
        other.x_desc = nullptr;
        other.y_desc = nullptr;
        other.w_desc = nullptr;
        other.b_desc = nullptr;
        other.act_desc = nullptr;
        other.conv_desc = nullptr;
#endif
        other.workspace_size = 0;
    }

    ~Opaque() {
#ifdef ENABLE_CUDNN_API
        CLEANUP_MCDNN_DESCRIPTORS();
#endif
    }

#ifdef ENABLE_CUDNN_API
    infiniStatus_t initializeMcdnnContext(ConvInfo &info,
                                          infiniDtype_t data_type,
                                          mcdnnDataType_t compute_type) {
        bool is_1d_conv = (info.ndim() == 1);
        int actual_tensor_ndim = is_1d_conv ? 4 : static_cast<int>(info.ndim() + 2);
        int spatial_ndim_for_conv_desc = is_1d_conv ? 2 : static_cast<int>(info.ndim());

        std::vector<int> input_dims_arr(actual_tensor_ndim);
        std::vector<int> output_dims_arr(actual_tensor_ndim);
        std::vector<int> filter_dims_arr(actual_tensor_ndim);
        std::vector<int> input_strides_arr(actual_tensor_ndim);
        std::vector<int> output_strides_arr(actual_tensor_ndim);

        initializeDimensionArrays(info, input_dims_arr, output_dims_arr,
                                  filter_dims_arr, input_strides_arr, output_strides_arr);

        std::vector<int> pads_arr(spatial_ndim_for_conv_desc);
        std::vector<int> strides_arr(spatial_ndim_for_conv_desc);
        std::vector<int> dilations_arr(spatial_ndim_for_conv_desc);

        initializeConvolutionParams(info, pads_arr, strides_arr, dilations_arr);

        mcdnnDataType_t mcdnn_data_type;
        CHECK_STATUS(getMcdnnDataType(data_type, mcdnn_data_type));

        CHECK_STATUS(createBasicDescriptors(input_dims_arr, output_dims_arr,
                                            filter_dims_arr, mcdnn_data_type, actual_tensor_ndim));

        CHECK_STATUS(createBiasDescriptors(info, mcdnn_data_type, actual_tensor_ndim));

        CHECK_STATUS(setupConvolutionDescriptor(pads_arr, strides_arr, dilations_arr,
                                                spatial_ndim_for_conv_desc, compute_type));

        if (info.bias_dims_size() == 0) {
            CHECK_STATUS(setupAlgorithmWithoutBias());
        } else {
            CHECK_STATUS(setupAlgorithmWithBias());
        }

        return INFINI_STATUS_SUCCESS;
    }
#endif

    static inline utils::Result<Opaque> create(
        std::shared_ptr<device::metax::Handle::Internal> internal_ptr,
        ConvInfo &info,
        infiniDtype_t data_type) {
#ifdef ENABLE_CUDNN_API
        Opaque opaque(internal_ptr);
        auto status = opaque.initializeMcdnnContext(info, data_type, MCDNN_DATA_FLOAT);
        if (status != INFINI_STATUS_SUCCESS) {
            return status;
        }
        return utils::Result<Opaque>(std::move(opaque));
#else
        return INFINI_STATUS_NOT_IMPLEMENTED;
#endif
    }
};

Descriptor::~Descriptor() {
    if (_opaque) {
        delete _opaque;
    }
}

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_,
    Descriptor **desc_ptr,
    infiniopTensorDescriptor_t y_desc,
    infiniopTensorDescriptor_t x_desc,
    infiniopTensorDescriptor_t w_desc,
    infiniopTensorDescriptor_t b_desc,
    const void *pads,
    const void *strides,
    const void *dilations,
    size_t n) {
#ifdef ENABLE_CUDNN_API
    auto handle = reinterpret_cast<device::metax::Handle *>(handle_);
    auto dtype = y_desc->dtype();

    CHECK_DTYPE(dtype, INFINI_DTYPE_F16, INFINI_DTYPE_F32, INFINI_DTYPE_BF16);

    auto result = ConvInfo::create(handle_, y_desc, x_desc, w_desc, b_desc,
                                   pads, strides, dilations, n);

    CHECK_RESULT(result);
    auto conv_info = result.take();
    auto opaque_result = Opaque::create(handle->internal(), conv_info, dtype);
    CHECK_RESULT(opaque_result);
    auto opaque = new Opaque(opaque_result.take());

    *desc_ptr = new Descriptor(
        dtype,
        std::move(conv_info),
        opaque->workspace_size,
        opaque,
        handle->device,
        handle->device_id);
    return INFINI_STATUS_SUCCESS;
#else
    return INFINI_STATUS_NOT_IMPLEMENTED;
#endif
}

infiniStatus_t Descriptor::calculate(
    void *workspace,
    size_t workspace_size,
    void *y,
    const void *x,
    const void *w,
    const void *bias,
    void *stream) const {
#ifdef ENABLE_CUDNN_API
    const float alpha = 1.0f, beta = 0.0f;
    if (bias != nullptr) {
        CHECK_STATUS(_opaque->internal->useMcdnn(
            (hcStream_t)stream, [&](hcdnnHandle_t handle) {
                CHECK_MCDNN(mcdnnConvolutionBiasActivationForward(
                    handle,
                    &alpha,
                    _opaque->x_desc,
                    x,
                    _opaque->w_desc,
                    w,
                    _opaque->conv_desc,
                    _opaque->algo,
                    workspace, workspace_size,
                    &beta,
                    _opaque->y_desc,
                    y,
                    _opaque->b_desc,
                    bias,
                    _opaque->act_desc,
                    _opaque->y_desc,
                    y));
                return INFINI_STATUS_SUCCESS;
            }));
    } else {
        CHECK_STATUS(_opaque->internal->useMcdnn(
            (hcStream_t)stream, [&](hcdnnHandle_t handle) {
                CHECK_MCDNN(mcdnnConvolutionForward(
                    handle,
                    &alpha,
                    _opaque->x_desc,
                    x,
                    _opaque->w_desc,
                    w,
                    _opaque->conv_desc,
                    _opaque->algo,
                    workspace, workspace_size,
                    &beta,
                    _opaque->y_desc,
                    y));
                return INFINI_STATUS_SUCCESS;
            }));
    }

    return INFINI_STATUS_SUCCESS;
#else
    return INFINI_STATUS_NOT_IMPLEMENTED;
#endif
}
} // namespace op::conv::metax
